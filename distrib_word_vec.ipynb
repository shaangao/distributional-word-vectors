{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter, defaultdict\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Distributional Counting (24pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 implement distributional counting (12pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrib_counting(doc, w, V, Vc, wv):\n",
    "\n",
    "    \"\"\" \n",
    "    inputs:\n",
    "        - doc: list of words; a **single** document to do distributional counting on\n",
    "        - w: integer; window size\n",
    "        - V: set of words; vocabulary\n",
    "        - Vc: set of words; context vocabulary\n",
    "        - wv: dict of counters; mapping from each V to Vc to counts\n",
    "    outputs: \n",
    "        none. the function mutates wv.\n",
    "    \"\"\"\n",
    "\n",
    "    # sliding window\n",
    "    for i, center in enumerate(doc):\n",
    "\n",
    "        # count only if the center word is in vocab\n",
    "        if center in V:\n",
    "\n",
    "            # get the start and end idx of window\n",
    "            start = max(0, i - w)\n",
    "            end = min(i + w + 1, len(doc))   # upper bound exclusive\n",
    "\n",
    "            # counting co-occurences\n",
    "            for j in range(start, end):\n",
    "\n",
    "                # skip center word\n",
    "                if j == i:\n",
    "                    continue\n",
    "\n",
    "                # increment counts if doc[j] is in contex vocab\n",
    "                if doc[j] in Vc:\n",
    "                    wv[center][doc[j]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'rest': Counter({'for': 2,\n",
       "                      'the': 3,\n",
       "                      'rest': 2,\n",
       "                      'of': 1,\n",
       "                      'day': 1}),\n",
       "             'for': Counter({'rest': 2, 'the': 1, 'of': 1}),\n",
       "             'the': Counter({'rest': 3,\n",
       "                      'for': 1,\n",
       "                      'of': 2,\n",
       "                      'the': 2,\n",
       "                      'day': 1}),\n",
       "             'of': Counter({'for': 1, 'the': 2, 'rest': 1, 'day': 1}),\n",
       "             'day': Counter({'rest': 1, 'of': 1, 'the': 1})})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check with toy example\n",
    "wv = defaultdict(Counter)\n",
    "distrib_counting(\n",
    "    doc=['rest', 'for', 'the', 'rest', 'of', 'the', 'day'],\n",
    "    w=3, \n",
    "    V=set(['day', 'for', 'of', 'rest', 'the']),\n",
    "    Vc=set(['day', 'for', 'of', 'rest', 'the']),\n",
    "    wv=wv\n",
    ")\n",
    "wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'rest': Counter({'for': 2, 'rest': 2, 'day': 1}),\n",
       "             'for': Counter({'rest': 2}),\n",
       "             'the': Counter({'rest': 3, 'for': 1, 'day': 1}),\n",
       "             'of': Counter({'for': 1, 'rest': 1, 'day': 1}),\n",
       "             'day': Counter({'rest': 1})})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check with toy example\n",
    "wv = defaultdict(Counter)\n",
    "distrib_counting(\n",
    "    doc=['rest', 'for', 'the', 'rest', 'of', 'the', 'day'],\n",
    "    w=3, \n",
    "    V=set(['day', 'for', 'of', 'rest', 'the']),\n",
    "    Vc=set(['day', 'for', 'rest']),\n",
    "    wv=wv\n",
    ")\n",
    "wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 run distributional counting on data (6pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus_path, V, Vc, w):\n",
    "\n",
    "    \"\"\" \n",
    "    apply distrib_counting() to each doc in corpus and return sparsely represented final word vecs.\n",
    "    inputs:\n",
    "        - corpus_path: string; path to txt corpus where each line is a space-seperated document\n",
    "        - V: set of words; vocabulary\n",
    "        - Vc: set of words; context vocabulary\n",
    "        - w: integer; window size\n",
    "    outputs: \n",
    "        - wv: sparsely represented final word vecs computed across all docs in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize co-occurence counts storage: V - Vc - counts\n",
    "    wv = defaultdict(Counter)\n",
    "\n",
    "    # process corpus\n",
    "    with open(corpus_path, 'r') as file:\n",
    "\n",
    "        for i, line in enumerate(file):\n",
    "\n",
    "            # # only process a subset of corpus for testing\n",
    "            # if i >= 10:\n",
    "            #     break\n",
    "\n",
    "            # # print progress\n",
    "            # if i % 100 == 0: print(f'document {i}')\n",
    "        \n",
    "            # split string into list of words\n",
    "            doc = line.split()\n",
    "            \n",
    "            # count co-occurences in doc\n",
    "            distrib_counting(\n",
    "                doc=doc,\n",
    "                w=w, \n",
    "                V=V,\n",
    "                Vc=Vc,\n",
    "                wv=wv\n",
    "            )\n",
    "\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15228"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vocab-15kws.txt\n",
    "vocab15kws_path = 'data/vocab-15kws.txt'\n",
    "with open(vocab15kws_path, 'r') as file:\n",
    "    vocab15kws = set(file.read().splitlines())\n",
    "len(vocab15kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vocab-5k.txt\n",
    "vocab5k_path = 'data/vocab-5k.txt'\n",
    "with open(vocab5k_path, 'r') as file:\n",
    "    vocab5k = set(file.read().splitlines())\n",
    "len(vocab5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set corpus path\n",
    "corpus_path = 'data/wiki-1percent.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 3\n",
    "wv_V15k_Vc5k_win3 = process_corpus(\n",
    "                        corpus_path=corpus_path,\n",
    "                        V=vocab15kws,\n",
    "                        Vc=vocab5k,\n",
    "                        w=3\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/wv_V15k_Vc5k_win3.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "joblib.dump(wv_V15k_Vc5k_win3, 'results/wv_V15k_Vc5k_win3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve co-occurence counts\n",
    "wv_V15k_Vc5k_win3['coffee']['coffee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 6\n",
    "wv_V15k_Vc5k_win6 = process_corpus(\n",
    "                        corpus_path=corpus_path,\n",
    "                        V=vocab15kws,\n",
    "                        Vc=vocab5k,\n",
    "                        w=6\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/wv_V15k_Vc5k_win6.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "joblib.dump(wv_V15k_Vc5k_win6, 'results/wv_V15k_Vc5k_win6.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve co-occurence counts\n",
    "wv_V15k_Vc5k_win6['coffee']['coffee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 EvalWS (6pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "\n",
    "    ab_dot = a@b\n",
    "    ab_norm = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "\n",
    "    if ab_norm:   # if both word vectors have length\n",
    "        return ab_dot / ab_norm\n",
    "    else:   # if either word has a 0-length word vector, define cos_sim as 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def cos_sim_sparse(w1, w2, Vc, wv_sparse):\n",
    "\n",
    "    \"\"\" \n",
    "    inputs:\n",
    "        - w1, w2: string; the two words to compute cosine similarity between\n",
    "        - Vc: set of words; context vocabulary\n",
    "        - wv_sparse: defaultdict; mapping V - Vc - counts\n",
    "    outputs: \n",
    "        - cos_sim between w1 and w2 computed using model wv_sparse\n",
    "    \"\"\"\n",
    "\n",
    "    ab_dot = 0\n",
    "    for w_context in Vc:   # we retrieve context word counts for w1 and w2 in the same iter so no need to convert Vc from set to list\n",
    "        ab_dot += wv_sparse[w1][w_context] * wv_sparse[w2][w_context]\n",
    "\n",
    "    ab_norm = np.linalg.norm(list(wv_sparse[w1].values())) * np.linalg.norm(list(wv_sparse[w2].values()))  # 0 entries does not matter for 2-norm\n",
    "\n",
    "    if ab_norm:   # if both word vectors have length\n",
    "        return ab_dot / ab_norm\n",
    "    else:   # if either word has a 0-length word vector, define cos_sim as 0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.895510424327929"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "cos_sim_sparse(w1='sun', w2='sunlight', Vc=vocab5k, wv_sparse=wv_V15k_Vc5k_win3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalWS(word_pairs, wv, Vc, true_sims):\n",
    "\n",
    "    \"\"\" \n",
    "    inputs:\n",
    "        - word_pairs: list of 2-word tuples to compute cosine similarity between\n",
    "        - wv: defaultdict; wv model to be evaluated, mapping V - Vc - counts\n",
    "        - Vc: set of words; context vocabulary\n",
    "        - true_sims: 1d array of ground-truth similarity of each word_pair\n",
    "    outputs: \n",
    "        - spearmanr between true_sims and wv_sims\n",
    "    \"\"\"\n",
    "\n",
    "    # get wv similarity for word_pairs\n",
    "    wv_sims = []\n",
    "    for w1, w2 in word_pairs:\n",
    "        wv_sims.append(cos_sim_sparse(w1=w1, w2=w2, Vc=Vc, wv_sparse=wv))\n",
    "    \n",
    "    # compute spearmanr between true_sims and wv_sims\n",
    "    return spearmanr(true_sims, wv_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simlex-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlex999_words len: 999; simlex999_sims len: 999\n"
     ]
    }
   ],
   "source": [
    "# load simlex-999\n",
    "simlex999_path = 'data/simlex-999.txt'\n",
    "\n",
    "simlex999_words, simlex999_sims = [], []\n",
    "with open(simlex999_path, 'r') as file:\n",
    "    for line in file.read().splitlines()[1:]:\n",
    "        w1, w2, sim = line.strip().split()\n",
    "        simlex999_words.append((w1, w2))\n",
    "        simlex999_sims.append(float(sim))\n",
    "simlex999_sims = np.array(simlex999_sims)\n",
    "\n",
    "print(f'simlex999_words len: {len(simlex999_words)}; simlex999_sims len: {len(simlex999_sims)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.05876135331349779, pvalue=0.06337563925440041)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalWS(word_pairs=simlex999_words, wv=wv_V15k_Vc5k_win3, Vc=vocab5k, true_sims=simlex999_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEN_words len: 3000; MEN_sims len: 3000\n"
     ]
    }
   ],
   "source": [
    "# load MEN\n",
    "MEN_path = 'data/men.txt'\n",
    "\n",
    "MEN_words, MEN_sims = [], []\n",
    "with open(MEN_path, 'r') as file:\n",
    "    for line in file.read().splitlines()[1:]:\n",
    "        w1, w2, sim = line.strip().split()\n",
    "        MEN_words.append((w1, w2))\n",
    "        MEN_sims.append(float(sim))\n",
    "MEN_sims = np.array(MEN_sims)\n",
    "\n",
    "print(f'MEN_words len: {len(MEN_words)}; MEN_sims len: {len(MEN_sims)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.2251396048448754, pvalue=8.800788745595221e-36)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalWS(word_pairs=MEN_words, wv=wv_V15k_Vc5k_win3, Vc=vocab5k, true_sims=MEN_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Inverse Document Frequency (IDF) (10pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus_path, Vc):\n",
    "\n",
    "    # for storing number of doc/sentences containing each word in context vocab\n",
    "    Vc_doc = defaultdict(int)\n",
    "\n",
    "    # process corpus to count Vc occurence\n",
    "    with open(corpus_path, 'r') as file:\n",
    "\n",
    "        for i, line in enumerate(file):\n",
    "        \n",
    "            # split string into list of words, and convert to set to remove duplicates\n",
    "            doc = set(line.split())\n",
    "            \n",
    "            # increment doc count for context vocab\n",
    "            for word in doc:\n",
    "                if word in Vc: Vc_doc[word] += 1\n",
    "\n",
    "    # get total num of docs in corpus\n",
    "    corpus_size = i + 1\n",
    "    print(corpus_size)\n",
    "\n",
    "    # compute IDF: corpus_size / freq\n",
    "    return {word: (corpus_size / doc_count) for word, doc_count in Vc_doc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997898\n"
     ]
    }
   ],
   "source": [
    "# compute IDF for Vc\n",
    "Vc_IDF = IDF(corpus_path, vocab5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(wv_TF, Vc_IDF):\n",
    "\n",
    "    \"\"\" \n",
    "    combine counts with IDF\n",
    "    \"\"\"\n",
    "\n",
    "    # for storing counts combined with IDF \n",
    "    wv_TF_IDF = defaultdict(Counter)\n",
    "\n",
    "    # iterate through every word pair in original count-based wv\n",
    "    for w_center, w_context_count in wv_TF.items():\n",
    "        for w_context, count in w_context_count.items():\n",
    "            wv_TF_IDF[w_center][w_context] = count * Vc_IDF[w_context]\n",
    "\n",
    "    return wv_TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TF_IDF for wv_V15k_Vc5k_win3\n",
    "wv_V15k_Vc5k_win3_IDF = TF_IDF(wv_TF=wv_V15k_Vc5k_win3, Vc_IDF=Vc_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.12778528309126566, pvalue=5.111621348174487e-05)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on simlex999\n",
    "evalWS(word_pairs=simlex999_words, wv=wv_V15k_Vc5k_win3_IDF, Vc=vocab5k, true_sims=simlex999_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.4388165624442915, pvalue=1.9006405060621283e-141)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on MEN\n",
    "evalWS(word_pairs=MEN_words, wv=wv_V15k_Vc5k_win3_IDF, Vc=vocab5k, true_sims=MEN_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Pointwise Mutual Information (PMI) (14pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Quantitative Comparisons (12pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Qualitative Analysis (25pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
